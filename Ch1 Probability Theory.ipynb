{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name: Chaeyoon Kim\n",
    "#City Email: Chaeyoon.Kim@city.ac.uk\n",
    "#Chris Bishop, \"Pattern Recognition and Machine Learning\", Springer, 2006 (https://g.co/kgs/CsLSX8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "\n",
    "A probability distribution expresses uncertainty about the outcome of an event. We often encode this uncertainty in a variable. In the *1st week tutorial question*, if we are considering the outcome of an event, $Y$, to be a coin toss, then we might consider $Y=1$ to be heads and $Y=0$ to be tails. We represent the probability of a given outcome with the notation:\n",
    "$$\n",
    "P(Y=1) = 0.5\n",
    "$$\n",
    "The first rule of probability is that the probability must normalize. The sum of the probability of all events must equal 1. So if the probability of heads ($Y=1$) is 0.5, then the probability of tails (the only other possible outcome) is given by\n",
    "$$\n",
    "P(Y=0) = 1-P(Y=1) = 0.5\n",
    "$$\n",
    "\n",
    "Probabilities are often defined as the limit of the ratio between the number of positive outcomes (e.g. *heads*) given the number of trials. If the number of positive outcomes for event $y$ is denoted by $n_y$ and the number of trials is denoted by $N$ then this gives the ratio \n",
    "$$\n",
    "P(Y=y) = \\lim_{N\\rightarrow \\infty}\\frac{n_y}{N}.\n",
    "$$\n",
    "In practice we never get to observe an event infinite times, so rather than considering this we often use the following estimate\n",
    "$$\n",
    "P(Y=y) \\approx \\frac{n_y}{N}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditioning\n",
    "\n",
    "When predicting whether a fruit boxes returns apples or oranges, we might think that this event is *independent* of the each boxes. If we include an observation such as a red box, then in a probability this is known as *conditioning*. We use this notation, $P(Y=y|X=x)$, to condition the outcome on a second variable (in this case red box or blue box). Or, often, for a shorthand we use $P(y|x)$ to represent this distribution (the $Y=$ and $X=$ being implicit). Because we don't believe either a coin toss or a fruit selection or whatever depends on case then we might write that \n",
    "$$\n",
    "P(y|x) = p(y).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a link to the exercise of the PoDS! (part1)\n",
    "Let's use this rule to compute the approximate probability that female passengers from the Tatanic dataset has aged 40 to 50 and paid more than or equal to 40."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Product Rule\n",
    "\n",
    "This number is the joint probability, $P(Y, X)$ which is much *smaller* than the conditional probability. The number can never be bigger than the conditional probability because it is computed using the *product rule*.\n",
    "$$\n",
    "p(Y=y, X=x) = p(Y=y|X=x)p(X=x)\n",
    "$$\n",
    "and $$p(X=x)$$ is a probability distribution, which is equal or less than 1, ensuring the joint distribution is typically smaller than the conditional distribution.\n",
    "\n",
    "The product rule is a *fundamental* rule of probability that *I must remember it!* It gives the relationship between the two questions: 1) What's the probability that female passengers was aged 40 to 50? and 2) What's the probability that female passengers has paid over or equal to 40?\n",
    "\n",
    "In our shorter notation we can write the product rule as\n",
    "$$\n",
    "p(y, x) = p(y|x)p(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sum Rule\n",
    "\n",
    "The other *fundamental rule* of probability is the *sum rule* this tells us how to get a *marginal* distribution from the joint distribution. Simply put it says that we need to sum across the value we'd like to remove.\n",
    "$$\n",
    "P(Y=y) = \\sum_{x} P(Y=y, X=x)\n",
    "$$\n",
    "Or in our shortened notation\n",
    "$$\n",
    "P(y) = \\sum_{x} P(y, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Rule\n",
    "\n",
    "Bayes rule is a very simple rule, don't be afraid it too much. It follows directly from the product rule of probability. Because $P(y, x) = P(y|x)P(x)$ and by symmetry $P(y,x)=P(x,y)=P(x|y)P(y)$ then by equating these two equations and dividing through by $P(y)$ we have\n",
    "$$\n",
    "P(x|y) = \\frac{P(y|x)P(x)}{P(y)}, \\text{ or } P(y|x) = \\frac{P(x|y)P(y)}{P(x)}\n",
    "$$\n",
    "which is known as Bayes' rule (or Bayes's rule, it depends how I choose to pronounce it). It's not difficult to derive, and its importance is more to do with the semantic operation that it enables. Each of these probability distributions represents the answer to a question we have about the world. Bayes rule (via the product rule) tells us how to *invert* the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
